#!/usr/bin/python -tt
# vim: set ts=4 sw=4 tw=79 et :

from ConfigParser import SafeConfigParser
from optparse import OptionParser
from StringIO import StringIO
from threading import Thread
import logging
import logging.handlers
import os
import os.path
import re
import sys
import time
import shlex
import signal

# ##########################################################
# ###  Do not edit this file, edit fluxios.cfg    #####

shutdown_flag = False # graceful shutdown

# initialize logger, and add console handler while we start up
log = logging.getLogger('log')
console_handler = logging.StreamHandler(sys.stderr)
log.setLevel(logging.INFO)
console_formatter = logging.Formatter('%(message)s')
console_handler.setFormatter(console_formatter)
log.addHandler(console_handler)
log.info("Fluxios starting...")

SIGNALS_TO_NAMES_DICT = dict((getattr(signal, n), n) \
    for n in dir(signal) if n.startswith('SIG') and '_' not in n )

# default configuration
default_cfg = StringIO("""\
[fluxios]
spool_directory = /var/spool/nagios/fluxios
log_file = /var/log/nagios/fluxios.log
log_max_size = 24
log_keep = 4
log_level = logging.INFO
interval = 15
measurement_prefix =
batch_size = 500
extra_tags = 

[influxdb]
host = 127.0.0.1
port = 8086
proxies = None

cluster = False
hosts = 
shuffle = True
healing_delay = 900

ssl = False
verify_ssl = True
timeout = 15
database = nagios
username = fluxios
password = 
use_udp = False
udp_port = 4444
""")

config_file = '/etc/fluxios/fluxios.cfg'

# config dictionary
cfg = {}

# influxdb client
db = 0

# options parsing
parser = OptionParser("usage: %prog [options] sends nagios performance data to InfluxDB.")
parser.add_option("-c", "--config", dest="config_file", default=config_file,
                    help="Set custom config file location.")


def convert_bool(value):
    """
    param value: string containing either "true" or "false", case insensitive
    return: boolean True or False, or the value if it's neither
    """
    if (value.lower() == "true"):
        return True
    elif (value.lower() == "false"):
        return False
    elif (value.lower() == "none" or value.strip() == ""):
        return None
    return value


def read_config(config_file, defaults):
    """
    param config_file: full path of file to read
    param defaults: string containing default options
    return: dict of configuration file sections and options
    """
    # initialize
    config = SafeConfigParser()

    # first, read config from defaults
    config.readfp(defaults)
        
    config_dict = {}

    # then read config from file
    if os.path.isfile(config_file):
        config.read(config_file)
    else:
        log.info("Could not read config file, using defaults: %s" % config_file)

    for section in config.sections():
        config_dict[section] = {}
        for name, value in config.items(section):
            config_dict[section][name] = convert_bool(value)
            log.debug("file config[%s]=%s" % (name, value))

    return config_dict


def config_logger():
    """
    sets up fluxios config
    """
    try:
        cfg['fluxios']['log_max_size'] = int(cfg['fluxios']['log_max_size'])
    except ValueError:
        print "log_max_size needs to be an integer"
        sys.exit(1)
    try:
        cfg['fluxios']['log_keep'] = int(cfg['fluxios']['log_keep'])
    except ValueError:
        print "log_keep needs to be an integer"
        sys.exit(1)

    log_max_bytes = cfg['fluxios']['log_max_size']*1024*1024

    try:
        file_handler = logging.handlers.RotatingFileHandler( \
            cfg['fluxios']['log_file'], \
            maxBytes=log_max_bytes, \
            backupCount=int(cfg['fluxios']['log_keep']), \
        )
    except IOError as e:
        print "IOError while configuring RotatingFileHandler: %s" % e
        sys.exit(1)

    formatter = logging.Formatter(
        "%(asctime)s %(filename)s[%(process)d] %(levelname)s (%(funcName)s) %(message)s",
        "%B %d %H:%M:%S")
    file_handler.setFormatter(formatter)
    log.addHandler(file_handler)
    log.info("Added file log (%s), removing console log handler" % \
        cfg['fluxios']['log_file'])
    log.removeHandler(console_handler)

def init_influxdb_client():
    global db

    if not cfg['influxdb']['cluster']:
        from influxdb import InfluxDBClient
        db = InfluxDBClient(
            host=cfg['influxdb']['host'],
            port=cfg['influxdb']['port'],
            username=cfg['influxdb']['username'],
            password=cfg['influxdb']['password'],
            database=cfg['influxdb']['database'],
            ssl=cfg['influxdb']['ssl'],
            verify_ssl=cfg['influxdb']['verify_ssl'],
            timeout=float(cfg['influxdb']['timeout']),
            use_udp=cfg['influxdb']['use_udp'],
            udp_port=cfg['influxdb']['udp_port'],
            proxies=cfg['influxdb']['proxies']
        )
    else:
        from influxdb import InfluxDBClusterClient
        db = InfluxDBClusterClient(
            hosts=cfg['influxdb']['hosts'],
            username=cfg['influxdb']['username'],
            password=cfg['influxdb']['password'],
            database=cfg['influxdb']['database'],
            ssl=cfg['influxdb']['ssl'],
            verify_ssl=cfg['influxdb']['verify_ssl'],
            timeout=float(cfg['influxdb']['timeout']),
            use_udp=cfg['influxdb']['use_udp'],
            udp_port=cfg['influxdb']['udp_port'],
            shuffle=cfg['influxdb']['shuffle'],
            healing_delay=cfg['influxdb']['healing_delay']
        )
   
 
def process_perfdata_file(file_name):
    '''
    param file: full path perfdata file to extract points from
    return: list of influxdb points
    '''

    processed_lines = 0  # number of perfdata lines processed
    skipped_lines = 0 # number of lines skipped because we couldn't massage them
    points = []
    
    perfdata_re = \
        "^([^=]+)=(U|[\d\.\-]+)([\w\/%]*);?([\d\.\-:~@]+)?;?([\d\.\-:~@]+)?;?([\d\.\-]+)?;?([\d\.\-]+)?;?\s*"

    try:
        file = open(file_name, "r")
        file_array = file.readlines()
        file.close()
    except (IOError, OSError) as ex:
        log.critical("Can't open file:%s error: %s" % (file, ex))
        return False
    # parse each line
    for line in file_array:
        processed_lines += 1
        try:
            line_dict = dict(re.split('::', x, 1) for x in line.split('\t'))
        except Exception as e:
            skipped_lines += 1
            log.warn("%s: Could not parse perfdata line into key::value pairs in file %s, skipping: %s" \
                % (e, file_name, line))
            continue

        # pick out values from the line
        if line_dict['DATATYPE'] == "SERVICEPERFDATA":
            service_description = line_dict['SERVICEDESC']
            perfdata = line_dict['SERVICEPERFDATA']
            if not perfdata:
                log.debug("perfdata string is empty while reading file %s from line %s" % \
                    (file, line))
                continue
            check_command = line_dict['SERVICECHECKCOMMAND'].split('!')[0]
        elif line_dict['DATATYPE'] == "HOSTPERFDATA":
            service_description = "__host__"
            perfdata = line_dict['HOSTPERFDATA']
            check_command = line_dict['HOSTCHECKCOMMAND'].split('!')[0]
        else:
            skipped_lines += 1
            log.warn("Unknown DATATYPE, skipping: '%s'" % \
                line_dict['DATATYPE'])
            continue
        
        host_name = line_dict['HOSTNAME']
        timestamp = line_dict['TIMET']

        # extract data from the perfdata string
        m = re.search(perfdata_re, perfdata)
        if m:
            (label, value, uom, warn, crit, min, max) = m.groups()
            field_candidates = {
                "label": label,
                "value": value,
                "uom": uom,
                "warn": warn,
                "crit": crit,
                "min": min,
                "max": max
            }
            # for the fields that contain something, float() them if necessary
            # and add them to the fields dict
            fields = {}
            for field, value in field_candidates.items():
                if value is not None and value.strip() != "":
                    if isinstance(value, (int, long)):
                        value = float(value)
                    fields[field] = value

            point = {
                "measurement": check_command,
                "timestamp": timestamp,
                "fields": fields,
                "tags": {
                    "service_description": service_description,
                    "host_name": host_name,
                    "metric": label,
                }
            }
            points.append(point)
        else:
            log.debug("perfdata string from file %s did not match, skipping: %s" % \
                (file, perfdata))

    log.info("Processed %s lines, skipped %s lines, into %s points", \
        processed_lines, skipped_lines, len(points))
    return points


def rm_file(file):
    """
    param file: File to be deleted
    """
    try:
        os.remove(file)
        return True
    except (OSError, IOError) as e:
        log.critical("Could not remove file %s error: %s" % (file, e))
        return False


def process_spool_dir(dir):
    """
    param dir: Directory containing perfdata files to be processed
    """
    log.info("Processing spool dir %s", dir)
    num_files = 0

    try:
        files = os.listdir(dir)
    except (IOError, OSError) as e:
        log.error("Exception reading spool dir(%s): %s" % (dir, e))
        return False
    for file in files:
        file_path = dir + '/' + file
        if check_skip_file(file_path):
            log.info("Skipping file: %s", file)
            continue

        num_files += 1
        points = process_perfdata_file(file_path)
        if send_points(points):
            log.info(("Successfully wrote {0} points to InfluxDB")
                .format(len(points)))
        else:
            log.error(("Losing {0} points due to error")
                .format(len(points)))

        if rm_file(file_path):
            log.debug(("Deleted file: {0}").format(file_path))
        else:
            log.debug(("Could not delete file: {0}").format(file_path))
    

def send_points(points):
    """
    param points: list of points to send to influxdb
    return: True on success, False otherwise
    """
    try:
        db.write_points(
            points, 
            time_precision='s', 
            tags=cfg['fluxios']['extra_tags'],
            batch_size=int(cfg['fluxios']['batch_size'])
        )
    except Exception as e:
        log.error(("Exception while trying to write points: {0}")
            .format(e))
        return False

    return True


def check_skip_file(file):
    """
    param file: Full path to file to check
    return: True if the file should be skipped, False otherwise
    """
    if (
        file == "host-perfdata" or
        file == "service-perfdata"
    ):
        return True
    elif re.match('^_', file):
        return True

    if os.stat(file)[6] == 0:
        log.info("Found empty file, deleting it: %s" % file)
        rm_file(file)
        return True

    if os.path.isdir(file):
        return True

    return False

def sighandler(signum, frame):
    log.info('Received %s, going to shutdown...' % \
        SIGNALS_TO_NAMES_DICT[signum])
    print "Received %s, going to shutdown..." % SIGNALS_TO_NAMES_DICT[signum]
    global shutdown_flag
    shutdown_flag = True
    sys.exit()


def loop():
    log.info("Starting main loop")
    # loop as long as we are not told to shut down
    while not shutdown_flag:
        try:
            process_spool_dir(cfg['fluxios']['spool_directory'])
            log.debug("sleeping for %s seconds" % cfg['fluxios']['interval'])
            time.sleep(float(cfg['fluxios']['interval']))
        except Exception as e:
            log.exception("Caught exception in loop()")
            time.sleep(float(cfg['fluxios']['interval']))

    log.info("fluxios shutting down")
    print "fluxios shutting down"
    sys.exit(0)


if __name__ == '__main__':
    (options, args) = parser.parse_args()
    cfg = read_config(options.config_file, default_cfg)
    config_logger()
    log.info("Configuration: {0}".format(cfg))
    
    # set up sighandler()
    signal.signal(signal.SIGTERM, sighandler)
    signal.signal(signal.SIGINT, sighandler)
    signal.signal(signal.SIGHUP, sighandler)

    init_influxdb_client()

    try:
        Thread(target=loop, args=()).start()
    except Exception as e:
        log.exception("Exception while trying to start thread")
        sys.exit(1)
   
    while True:
        time.sleep(1) 

    sys.exit(0)
